# elk/logstash/config/logstash.conf
# Logstash pipeline configuration for processing FastAPI application logs.

input {
  # Input from Filebeat (or directly from Docker logs via stdout/stderr)
  # For Docker Compose, the FastAPI app logs to stdout, and Logstash can pick it up
  # if configured to read from a file or a Docker log driver.
  # For this setup, we assume the FastAPI app is sending logs via a Beats-like mechanism
  # or Logstash is configured to read from a file/socket.
  # A simpler approach for Docker Compose is to let Logstash read from a named pipe or a shared volume.
  # However, the most robust way for centralized logging is using Filebeat on the host or a sidecar.
  # For this example, we'll simulate a Beats input, assuming Filebeat is forwarding logs.
  # In a real Docker setup, you might use a Docker logging driver (e.g., GELF, Fluentd)
  # or a dedicated Filebeat container.
  
  # For simplicity, we'll assume the FastAPI app's stdout/stderr is being collected
  # by a Filebeat instance (either on host or as a sidecar) and sent to Logstash's Beats input.
  # Alternatively, Logstash can directly consume from a TCP input if the app sends raw JSON.
  
  beats {
    port => 5044
    ssl => false # Set to true and configure certificates for production
  }
}

filter {
  # Parse JSON logs from the FastAPI application
  json {
    source => "message"
    # If the log message is not pure JSON, you might need to target a specific field
    # or use a grok filter first to extract the JSON part.
    # For python-json-logger, the entire message is JSON.
  }

  # Add fields for better indexing and querying in Elasticsearch
  mutate {
    add_field => { "[@metadata][pipeline]" => "fastapi_app_logs" }
    add_field => { "application" => "fastapi_app" }
    add_field => { "environment" => "${APP_ENV:development}" } # Use environment variable for env
    # Rename fields for consistency or to avoid conflicts
    rename => { "levelname" => "log_level" }
    rename => { "asctime" => "timestamp" }
    rename => { "name" => "logger_name" }
  }

  # Convert timestamp string to Logstash's @timestamp field
  date {
    match => ["timestamp", "YYYY-MM-DD HH:mm:ss,SSS"]
    target => "@timestamp"
    remove_field => ["timestamp"] # Remove original timestamp field
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["host", "port", "ecs", "agent", "input", "log", "message"]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "fastapi-logs-%{+YYYY.MM.dd}" # Daily index for logs
    user => "logstash_system" # Use the built-in Logstash system user
    password => "${LOGSTASH_PASSWORD}"
    ssl => true # Enable SSL for production
    cacert => "/usr/share/logstash/config/certs/ca.crt" # Path to CA certificate if using custom certs
    # For a simple Docker Compose setup, you might start with ssl => false,
    # but enable and configure certificates for production.
    # For this setup, we rely on the default self-signed certs generated by ES.
    # Logstash needs to trust the ES certificate.
    # In a production setup, you'd mount a volume with your CA certs.
    # For this example, we'll assume the default self-signed certs are trusted or SSL is disabled for simplicity.
    # If SSL is true, you might need to add `ssl_certificate_verification => false` for self-signed certs in dev.
    # For production, always verify certificates.
  }

  # Output to stdout for debugging purposes (remove in production)
  stdout {
    codec => rubydebug
  }
}