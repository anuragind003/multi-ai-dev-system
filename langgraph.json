{
  "app": {
    "title": "Multi-AI Development System",
    "version": "1.0.0",
    "description": "Automated software development using specialized AI agents",
    "api_base_url": "http://127.0.0.1:8000"
  },
  "dev": {
    "port": 3001,
    "host": "localhost",
    "watch_paths": [
      "graph.py",
      "graph_nodes.py",
      "async_graph.py",
      "async_graph_nodes.py",
      "agents/"
    ],
    "reload": true,
    "allow_blocking": false,
    "enable_api": true
  },
  "graphs": {
    "phased_workflow": "async_graph:create_async_phased_workflow",
    "iterative_workflow": "async_graph:create_async_iterative_workflow",
    "basic_workflow": "async_graph:create_async_basic_workflow",
    "modular_workflow": "async_graph:create_async_modular_workflow",
    "resumable_workflow": "async_graph:create_async_resumable_workflow",
    "implementation_workflow": "async_graph:create_async_implementation_workflow",
    
    "sync_phased": "graph:create_phased_workflow",
    "sync_iterative": "graph:create_iterative_workflow",
    "sync_basic": "graph:create_basic_workflow",
    "sync_modular": "graph:create_modular_workflow",
    "sync_resumable": "graph:create_resumable_workflow",
    "sync_implementation": "graph:create_implementation_workflow"
  },
  "dependencies": [
    "agents",
    "agent_state",
    "graph_nodes",
    "async_graph_nodes",
    "shared_memory",
    "monitoring",
    "rag_manager",
    "config",
    "tools"
  ],
  "config": {
    "llm_provider": "gemini",
    "environment": "development",
    "temperature_strategy": {
      "analytical": 0.2,
      "creative": 0.4,
      "code_generation": 0.1
    },
    "monitoring": {
      "enabled": true,
      "log_level": "info"
    },
    "memory": {
      "type": "shared_project_memory",
      "batch_size": 50
    }
  }
}
